{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semisupervised classification on CWRU dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.manual_seed(13)\n",
    "torch.cuda.manual_seed(13)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = './data/raw'\n",
    "NP_PATH = './data/np'\n",
    "SAVE_PATH = './saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_P = 0.005  # 0.5% labels are given\n",
    "SCREEN = False  # screen some combination of (label, diameter) if set as true\n",
    "SCREEN_LABEL = 1\n",
    "SCREEN_DIAM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "BATCH = 32\n",
    "FRAME_LEN = 1024\n",
    "TOL = 100\n",
    "EPOCH = 10000\n",
    "\n",
    "PRETRAIN = True\n",
    "PRETRAIN_EPOCH = 10000\n",
    "PRE_LR = 1e-4\n",
    "\n",
    "BETA1=1\n",
    "BETA2=100\n",
    "BETA3=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diameter(n):\n",
    "    if n<=100:\n",
    "        return 0\n",
    "    elif n<169:\n",
    "        return 1\n",
    "    elif n<209:\n",
    "        return 2\n",
    "    elif n<3000:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_diameter(n):\n",
    "    if n<=100:\n",
    "        return 0\n",
    "    elif n<169:\n",
    "        return 1\n",
    "    elif n<209:\n",
    "        return 2\n",
    "    elif n<3000:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "def _load_preprocessed(np_path):\n",
    "    DATA = np.load('%s/data.npy'%np_path)\n",
    "    train_idx = np.load('%s/train_idx.npy'%np_path)\n",
    "    test_idx = np.load('%s/test_idx.npy'%np_path)\n",
    "    trainY = np.load('%s/train_labels.npy'%np_path)\n",
    "    testY = np.load('%s/test_labels.npy'%np_path)\n",
    "    train_diameter = np.load('%s/train_diameter.npy'%np_path)\n",
    "    test_diameter = np.load('%s/test_diameter.npy'%np_path)\n",
    "    train_rpm = np.load('%s/train_rpm.npy'%np_path)\n",
    "    test_rpm = np.load('%s/test_rpm.npy'%np_path)\n",
    "    return DATA[:,:1], train_idx, test_idx, trainY, testY, train_diameter, test_diameter, train_rpm, test_rpm\n",
    "def _preprocess(raw_path, out_path, frame_len=1024, frame_intv=512):\n",
    "    dirnames = glob('%s/*'%raw_path)\n",
    "    dirnames.sort()\n",
    "    data = {}\n",
    "    data['DE']=[]\n",
    "    data['FE']=[]\n",
    "    data['BA']=[]\n",
    "    train_idcs=[]\n",
    "    test_idcs=[]\n",
    "    train_labels=[]\n",
    "    test_labels=[]\n",
    "    train_rpms=[]\n",
    "    test_rpms=[]\n",
    "    cnt=0\n",
    "    RPM_LIST=[1797,1772,1750,1730]\n",
    "    train_fault_scale=[]\n",
    "    test_fault_scale=[]\n",
    "    for dir in dirnames:\n",
    "        l = int(os.path.basename(dir)[0])\n",
    "        fnames = glob(dir+'/*.mat')\n",
    "        fnames.sort()\n",
    "        for f in fnames:    \n",
    "            f_id = int(os.path.basename(f).split('.')[0])\n",
    "            fault = _find_diameter(f_id)\n",
    "            mat = io.loadmat(f)\n",
    "            r=0\n",
    "            s=0\n",
    "            fl={}\n",
    "            for m in ['DE','FE','BA']:\n",
    "                fl[m]=False\n",
    "            for k in mat:\n",
    "                if k[-3:] == 'RPM':\n",
    "                    r=mat[k][0,0]\n",
    "                if k[-4:] == 'time' and k[:4] == 'X098' and 'X%03d'%f_id=='X099':\n",
    "                    continue\n",
    "                if k[-4:]=='time':\n",
    "                    data[k[5:7]].append(mat[k])\n",
    "                    fl[k[5:7]]=True\n",
    "                    if s>0 and s!= mat[k].shape[0]:\n",
    "                        print(\"This is NOT expected\")\n",
    "\n",
    "                    s=mat[k].shape[0]\n",
    "            for m in ['DE','FE','BA']:\n",
    "                if not fl[m]:\n",
    "                    data[m].append(np.zeros((s,1)))\n",
    "            idx = np.arange(cnt, cnt+s-frame_len, frame_intv)\n",
    "\n",
    "            train_idx,test_idx = np.split(idx, [(idx.shape[0]*4)//5])\n",
    "            train_idcs.append(train_idx)\n",
    "            test_idcs.append(test_idx)\n",
    "            train_labels.append(np.ones_like(train_idx)*l)\n",
    "            test_labels.append(np.ones_like(test_idx)*l)\n",
    "            train_fault_scale.append(np.ones_like(train_idx)*fault)\n",
    "            test_fault_scale.append(np.ones_like(test_idx)*fault)\n",
    "            if r==0:\n",
    "                r=RPM_LIST[(f_id-1)%4]\n",
    "            trr = np.ones(train_idx.shape[0])*r\n",
    "            train_rpms.append(trr)\n",
    "            test_rpms.append(np.ones_like(test_idx)*r)\n",
    "            cnt+=s\n",
    "    for d in data:\n",
    "        data[d] = np.concatenate(data[d],axis=0)\n",
    "    X=np.concatenate(list(data.values()),axis=-1)\n",
    "    train_indices = np.concatenate(train_idcs)\n",
    "    test_indices = np.concatenate(test_idcs)\n",
    "    train_rpms = np.concatenate(train_rpms)\n",
    "    test_rpms = np.concatenate(test_rpms)\n",
    "    train_labels = np.concatenate(train_labels)\n",
    "    test_labels = np.concatenate(test_labels)\n",
    "    train_fault_scale = np.concatenate(train_fault_scale)\n",
    "    test_fault_scale = np.concatenate(test_fault_scale)\n",
    "    np.save('%s/data.npy'%out_path,X)\n",
    "    np.save('%s/train_idx.npy'%out_path,train_indices)\n",
    "    np.save('%s/test_idx.npy'%out_path,test_indices)\n",
    "    np.save('%s/train_rpm.npy'%out_path,train_rpms)\n",
    "    np.save('%s/test_rpm.npy'%out_path,test_rpms)\n",
    "    np.save('%s/train_labels.npy'%out_path, train_labels)\n",
    "    np.save('%s/test_labels.npy'%out_path, test_labels)\n",
    "    np.save('%s/train_diameter.npy'%out_path, train_fault_scale)\n",
    "    np.save('%s/test_diameter.npy'%out_path, test_fault_scale)\n",
    "    return X[:,:1], train_indices, test_indices, train_labels, test_labels, train_fault_scale, test_fault_scale, train_rpms, test_rpms\n",
    "\n",
    "def get_preprocessed_data(raw_path, np_path, frame_len, frame_intv):\n",
    "    '''\n",
    "    If data has already been preprocessed, load preprocessed data.\n",
    "    else, read and preprocess data.\n",
    "    '''\n",
    "    npy_files = glob(\"%s/*.npy\"%np_path)\n",
    "    npy_files = [os.path.basename(x) for x in npy_files]\n",
    "    if set(['data.npy','train_idx.npy','test_idx.npy','train_rpm.npy','test_rpm.npy','train_labels.npy','test_labels.npy','train_diameter.npy','test_diameter.npy']).issubset(set(npy_files)):\n",
    "        return _load_preprocessed(np_path)\n",
    "    else:\n",
    "        return _preprocess(raw_path, np_path, frame_len, frame_intv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cwru(raw_path, out_path):\n",
    "    get_preprocessed_data(raw_path, out_path, frame_len=1024, frame_intv=204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_cwru(RAW_PATH, NP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "DATA : (total len of data, 3)\n",
    "    - each column corresponds to DE/FE/BA data\n",
    "train_idx/test_idx : (# of frames)\n",
    "    - index of starting points of frames(1 frame : 1024 points)\n",
    "trainY/testY : (# of frames)\n",
    "    - labels(0: normal, 1: inner, 2: ball, 3: outer_centered, 4: outer_orthogonal, 5: outer_opposite)\n",
    "train_diameter/test_diameter : (# of frames)\n",
    "    - fault diameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = np.load('%s/data.npy'%NP_PATH)\n",
    "train_idx = np.load('%s/train_idx.npy'%NP_PATH)\n",
    "test_idx = np.load('%s/test_idx.npy'%NP_PATH)\n",
    "trainY = np.load('%s/train_labels.npy'%NP_PATH)\n",
    "testY = np.load('%s/test_labels.npy'%NP_PATH)\n",
    "train_diameter = np.load('%s/train_diameter.npy'%NP_PATH)\n",
    "test_diameter = np.load('%s/test_diameter.npy'%NP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9012305, 3) (35082,) (8803,) (35082,) (8803,) (35082,) (8803,)\n"
     ]
    }
   ],
   "source": [
    "print(DATA.shape, train_idx.shape, test_idx.shape, trainY.shape, testY.shape, train_diameter.shape, test_diameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = np.random.choice(trainY.shape[0], trainY.shape[0], replace=False) < int(MASK_P*trainY.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCREEN:\n",
    "    #TBD : masked ratio is not maintained\n",
    "    MASK[(trainY==SCREEN_LABEL)&(train_diameter==SCREEN_DIAM)] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shuffle = np.arange(train_idx.shape[0])\n",
    "if PRETRAIN:\n",
    "    pretrain_shuffle = np.arange(train_idx[MASK].shape[0])\n",
    "np.random.shuffle(train_shuffle)\n",
    "np.random.shuffle(pretrain_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_jh import ENC\n",
    "param = {\n",
    "    'in_dim' : 1,\n",
    "    'h_dim' : 32,\n",
    "    'n_mode' : 3,\n",
    "    'enc_depth' : 4,\n",
    "    'n_label' : 6\n",
    "}\n",
    "model = ENC(**param).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "if PRETRAIN:\n",
    "    optimizer_pretrain = optim.Adam(model.parameters(), lr=PRE_LR)\n",
    "best_eval = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_step(model, X, Y, M=None, train=False, optimizer=None):\n",
    "    if M is None:\n",
    "        M = torch.zeros_like(Y) == 0\n",
    "        M = M.to(device=DEVICE, dtype=torch.bool)\n",
    "    \n",
    "    m = X.sum(dim=-1) != 0\n",
    "    if train:\n",
    "        model.zero_grad()\n",
    "    recon_loss, class_loss, entropy_loss, pred = model(X,m,Y,M)\n",
    "    correct = (Y[Y==pred]).shape[0]\n",
    "    wrong = (Y[Y!=pred]).shape[0]\n",
    "    \n",
    "    total_loss = BETA1*recon_loss + BETA2*class_loss + BETA3*entropy_loss\n",
    "    if train:\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss, correct, wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain (if set as true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,train_idx,trainY,shuffle,epoch,pretrain=False):\n",
    "    patience = 0\n",
    "    best_eval = float('inf')\n",
    "    for e in range(epoch):\n",
    "        train_loss = 0.0\n",
    "        n = 0\n",
    "        train_correct = 0\n",
    "        train_wrong = 0\n",
    "        model.train()\n",
    "        timestamp = time.time()\n",
    "        for b in range((shuffle.shape[0]-1)//BATCH+1):\n",
    "            X = DATA[train_idx[shuffle[b*BATCH:(b+1)*BATCH]][:,None]+np.arange(FRAME_LEN)]\n",
    "            Y = trainY[shuffle[b*BATCH:(b+1)*BATCH]]\n",
    "            X = np.transpose(X, [0,2,1])\n",
    "            X = torch.tensor(X, device=DEVICE, dtype=torch.float)\n",
    "            Y = torch.tensor(Y, device=DEVICE, dtype=torch.long)\n",
    "            M=None\n",
    "            if not pretrain:\n",
    "                M = MASK[shuffle[b*BATCH:(b+1)*BATCH]]\n",
    "                M = torch.tensor(M, device=DEVICE, dtype=torch.bool)\n",
    "            loss, correct, wrong = batch_step(model, X, Y, train=True, optimizer=optimizer)\n",
    "            train_loss = (train_loss*n + loss.item()*X.shape[0])/(n+X.shape[0])\n",
    "            n += X.shape[0]\n",
    "            train_correct += correct\n",
    "            train_wrong += wrong\n",
    "        model.eval()\n",
    "        eval_loss = 0.0\n",
    "        n = 0\n",
    "        eval_correct = 0\n",
    "        eval_wrong = 0\n",
    "        with torch.no_grad():\n",
    "            for b in range((test_idx.shape[0]-1)//BATCH+1):\n",
    "                X = DATA[test_idx[b*BATCH:(b+1)*BATCH][:,None]+np.arange(FRAME_LEN)]\n",
    "                Y = testY[b*BATCH:(b+1)*BATCH]\n",
    "                X = np.transpose(X, [0,2,1])\n",
    "                X = torch.tensor(X, device=DEVICE, dtype=torch.float)\n",
    "                Y = torch.tensor(Y, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "                loss, correct, wrong = batch_step(model, X, Y, train=False)\n",
    "                eval_loss = (eval_loss*n + loss.item()*X.shape[0])/(n+X.shape[0])\n",
    "                n+= X.shape[0]\n",
    "                eval_correct += correct\n",
    "                eval_wrong += wrong\n",
    "        mode = 'Pretrain' if pretrain else 'Train'\n",
    "        print('(%.2fs)/[%s %d]\\n\\t(train) loss : %.5f,\\tacc : %.5f'%(time.time()-timestamp, mode,e+1, train_loss, train_correct/(train_correct+train_wrong)))\n",
    "        print('\\t(eval) loss : %.5f,\\tacc : %.5f'%(eval_loss, eval_correct/(eval_correct+eval_wrong)))\n",
    "        if eval_loss < best_eval:\n",
    "            best_eval = eval_loss\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(), '%s/%s_best.pth'%(SAVE_PATH,mode))\n",
    "        patience += 1\n",
    "        if patience>TOL:\n",
    "            print('Early stop at Epoch %d'%(e+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9.06s)/[Pretrain 1]\n",
      "\t(train) loss : 209.12918,\tacc : 0.20571\n",
      "\t(eval) loss : 209.12376,\tacc : 0.22436\n",
      "(7.32s)/[Pretrain 2]\n",
      "\t(train) loss : 209.06720,\tacc : 0.27429\n",
      "\t(eval) loss : 209.07051,\tacc : 0.21652\n",
      "(6.29s)/[Pretrain 3]\n",
      "\t(train) loss : 209.00255,\tacc : 0.26857\n",
      "\t(eval) loss : 209.00995,\tacc : 0.21595\n",
      "(6.19s)/[Pretrain 4]\n",
      "\t(train) loss : 208.93387,\tacc : 0.26857\n",
      "\t(eval) loss : 208.94321,\tacc : 0.21413\n",
      "(5.98s)/[Pretrain 5]\n",
      "\t(train) loss : 208.85924,\tacc : 0.25143\n",
      "\t(eval) loss : 208.87182,\tacc : 0.21493\n",
      "(6.10s)/[Pretrain 6]\n",
      "\t(train) loss : 208.78052,\tacc : 0.28000\n",
      "\t(eval) loss : 208.79726,\tacc : 0.21470\n",
      "(6.10s)/[Pretrain 7]\n",
      "\t(train) loss : 208.70025,\tacc : 0.26286\n",
      "\t(eval) loss : 208.71558,\tacc : 0.21686\n",
      "(6.06s)/[Pretrain 8]\n",
      "\t(train) loss : 208.61156,\tacc : 0.28000\n",
      "\t(eval) loss : 208.62921,\tacc : 0.29433\n",
      "(6.10s)/[Pretrain 9]\n",
      "\t(train) loss : 208.52159,\tacc : 0.28000\n",
      "\t(eval) loss : 208.53663,\tacc : 0.26127\n",
      "(6.13s)/[Pretrain 10]\n",
      "\t(train) loss : 208.42587,\tacc : 0.27429\n",
      "\t(eval) loss : 208.43639,\tacc : 0.24389\n",
      "(6.15s)/[Pretrain 11]\n",
      "\t(train) loss : 208.31880,\tacc : 0.28000\n",
      "\t(eval) loss : 208.32453,\tacc : 0.25696\n",
      "(5.97s)/[Pretrain 12]\n",
      "\t(train) loss : 208.20466,\tacc : 0.28000\n",
      "\t(eval) loss : 208.20145,\tacc : 0.24991\n",
      "(6.06s)/[Pretrain 13]\n",
      "\t(train) loss : 208.08202,\tacc : 0.28000\n",
      "\t(eval) loss : 208.06252,\tacc : 0.26059\n",
      "(5.99s)/[Pretrain 14]\n",
      "\t(train) loss : 207.94004,\tacc : 0.29143\n",
      "\t(eval) loss : 207.90238,\tacc : 0.26037\n",
      "(5.96s)/[Pretrain 15]\n",
      "\t(train) loss : 207.78142,\tacc : 0.29143\n",
      "\t(eval) loss : 207.71048,\tacc : 0.27082\n",
      "(6.09s)/[Pretrain 16]\n",
      "\t(train) loss : 207.61370,\tacc : 0.29143\n",
      "\t(eval) loss : 207.48520,\tacc : 0.27445\n",
      "(6.10s)/[Pretrain 17]\n",
      "\t(train) loss : 207.40931,\tacc : 0.29714\n",
      "\t(eval) loss : 207.21294,\tacc : 0.28331\n",
      "(6.59s)/[Pretrain 18]\n",
      "\t(train) loss : 207.16437,\tacc : 0.30857\n",
      "\t(eval) loss : 206.88062,\tacc : 0.29694\n",
      "(6.48s)/[Pretrain 19]\n",
      "\t(train) loss : 206.85475,\tacc : 0.31429\n",
      "\t(eval) loss : 206.45259,\tacc : 0.30444\n",
      "(6.49s)/[Pretrain 20]\n",
      "\t(train) loss : 206.45112,\tacc : 0.32000\n",
      "\t(eval) loss : 205.89972,\tacc : 0.31364\n",
      "(6.68s)/[Pretrain 21]\n",
      "\t(train) loss : 206.04783,\tacc : 0.31429\n",
      "\t(eval) loss : 205.20752,\tacc : 0.30399\n",
      "(6.48s)/[Pretrain 22]\n",
      "\t(train) loss : 205.45155,\tacc : 0.29143\n",
      "\t(eval) loss : 204.29122,\tacc : 0.28331\n",
      "(6.74s)/[Pretrain 23]\n",
      "\t(train) loss : 204.75179,\tacc : 0.26857\n",
      "\t(eval) loss : 203.12695,\tacc : 0.27025\n",
      "(6.45s)/[Pretrain 24]\n",
      "\t(train) loss : 203.63785,\tacc : 0.25714\n",
      "\t(eval) loss : 201.63884,\tacc : 0.25764\n",
      "(6.31s)/[Pretrain 25]\n",
      "\t(train) loss : 202.46152,\tacc : 0.26286\n",
      "\t(eval) loss : 199.78148,\tacc : 0.24776\n",
      "(6.06s)/[Pretrain 26]\n",
      "\t(train) loss : 200.69078,\tacc : 0.25714\n",
      "\t(eval) loss : 197.74390,\tacc : 0.21788\n",
      "(6.08s)/[Pretrain 27]\n",
      "\t(train) loss : 198.82364,\tacc : 0.24571\n",
      "\t(eval) loss : 195.46694,\tacc : 0.21606\n",
      "(6.11s)/[Pretrain 28]\n",
      "\t(train) loss : 196.55107,\tacc : 0.24571\n",
      "\t(eval) loss : 193.25602,\tacc : 0.21606\n",
      "(6.00s)/[Pretrain 29]\n",
      "\t(train) loss : 194.27859,\tacc : 0.24571\n",
      "\t(eval) loss : 191.25029,\tacc : 0.21652\n",
      "(6.08s)/[Pretrain 30]\n",
      "\t(train) loss : 191.90882,\tacc : 0.25143\n",
      "\t(eval) loss : 189.85906,\tacc : 0.23185\n",
      "(5.97s)/[Pretrain 31]\n",
      "\t(train) loss : 189.61847,\tacc : 0.28000\n",
      "\t(eval) loss : 188.37245,\tacc : 0.25185\n",
      "(6.40s)/[Pretrain 32]\n",
      "\t(train) loss : 188.26782,\tacc : 0.25143\n",
      "\t(eval) loss : 186.26620,\tacc : 0.24333\n",
      "(6.08s)/[Pretrain 33]\n",
      "\t(train) loss : 185.91352,\tacc : 0.27429\n",
      "\t(eval) loss : 183.32571,\tacc : 0.38828\n",
      "(6.07s)/[Pretrain 34]\n",
      "\t(train) loss : 184.08016,\tacc : 0.37143\n",
      "\t(eval) loss : 179.66152,\tacc : 0.40486\n",
      "(6.11s)/[Pretrain 35]\n",
      "\t(train) loss : 180.46670,\tacc : 0.37143\n",
      "\t(eval) loss : 175.73098,\tacc : 0.40009\n",
      "(5.97s)/[Pretrain 36]\n",
      "\t(train) loss : 178.19444,\tacc : 0.36000\n",
      "\t(eval) loss : 170.86420,\tacc : 0.40145\n",
      "(6.24s)/[Pretrain 37]\n",
      "\t(train) loss : 172.43689,\tacc : 0.38286\n",
      "\t(eval) loss : 164.70387,\tacc : 0.39736\n",
      "(6.37s)/[Pretrain 38]\n",
      "\t(train) loss : 168.26117,\tacc : 0.38286\n",
      "\t(eval) loss : 158.44020,\tacc : 0.39396\n",
      "(6.18s)/[Pretrain 39]\n",
      "\t(train) loss : 165.78376,\tacc : 0.36000\n",
      "\t(eval) loss : 154.31210,\tacc : 0.40327\n",
      "(6.28s)/[Pretrain 40]\n",
      "\t(train) loss : 162.77322,\tacc : 0.37143\n",
      "\t(eval) loss : 151.55173,\tacc : 0.40498\n",
      "(6.22s)/[Pretrain 41]\n",
      "\t(train) loss : 159.54101,\tacc : 0.38286\n",
      "\t(eval) loss : 150.48492,\tacc : 0.40225\n",
      "(6.29s)/[Pretrain 42]\n",
      "\t(train) loss : 159.58926,\tacc : 0.37143\n",
      "\t(eval) loss : 149.62408,\tacc : 0.40373\n",
      "(6.31s)/[Pretrain 43]\n",
      "\t(train) loss : 159.74867,\tacc : 0.36000\n",
      "\t(eval) loss : 148.75670,\tacc : 0.40520\n",
      "(6.21s)/[Pretrain 44]\n",
      "\t(train) loss : 158.05936,\tacc : 0.37714\n",
      "\t(eval) loss : 147.95573,\tacc : 0.40214\n",
      "(6.14s)/[Pretrain 45]\n",
      "\t(train) loss : 157.42800,\tacc : 0.38286\n",
      "\t(eval) loss : 147.55186,\tacc : 0.40747\n",
      "(5.99s)/[Pretrain 46]\n",
      "\t(train) loss : 155.42751,\tacc : 0.40000\n",
      "\t(eval) loss : 147.58471,\tacc : 0.40611\n",
      "(6.18s)/[Pretrain 47]\n",
      "\t(train) loss : 157.00973,\tacc : 0.38286\n",
      "\t(eval) loss : 147.98226,\tacc : 0.40066\n",
      "(6.01s)/[Pretrain 48]\n",
      "\t(train) loss : 157.47268,\tacc : 0.37143\n",
      "\t(eval) loss : 146.46035,\tacc : 0.40793\n",
      "(5.97s)/[Pretrain 49]\n",
      "\t(train) loss : 155.48380,\tacc : 0.36571\n",
      "\t(eval) loss : 145.60725,\tacc : 0.41520\n",
      "(6.06s)/[Pretrain 50]\n",
      "\t(train) loss : 154.61972,\tacc : 0.38286\n",
      "\t(eval) loss : 145.54233,\tacc : 0.41531\n",
      "(5.99s)/[Pretrain 51]\n",
      "\t(train) loss : 154.72968,\tacc : 0.38857\n",
      "\t(eval) loss : 145.71143,\tacc : 0.40611\n",
      "(5.99s)/[Pretrain 52]\n",
      "\t(train) loss : 154.06631,\tacc : 0.40000\n",
      "\t(eval) loss : 145.56690,\tacc : 0.40702\n",
      "(6.00s)/[Pretrain 53]\n",
      "\t(train) loss : 153.74109,\tacc : 0.39429\n",
      "\t(eval) loss : 145.36059,\tacc : 0.40759\n",
      "(6.15s)/[Pretrain 54]\n",
      "\t(train) loss : 154.03381,\tacc : 0.37714\n",
      "\t(eval) loss : 145.21897,\tacc : 0.40520\n",
      "(6.11s)/[Pretrain 55]\n",
      "\t(train) loss : 153.39196,\tacc : 0.38286\n",
      "\t(eval) loss : 144.53144,\tacc : 0.41122\n",
      "(6.04s)/[Pretrain 56]\n",
      "\t(train) loss : 152.41897,\tacc : 0.37143\n",
      "\t(eval) loss : 144.64063,\tacc : 0.40566\n",
      "(6.12s)/[Pretrain 57]\n",
      "\t(train) loss : 152.15366,\tacc : 0.43429\n",
      "\t(eval) loss : 144.37306,\tacc : 0.40270\n",
      "(6.06s)/[Pretrain 58]\n",
      "\t(train) loss : 153.42252,\tacc : 0.36571\n",
      "\t(eval) loss : 143.73433,\tacc : 0.40963\n",
      "(6.05s)/[Pretrain 59]\n",
      "\t(train) loss : 151.10169,\tacc : 0.40000\n",
      "\t(eval) loss : 143.39958,\tacc : 0.40214\n",
      "(6.13s)/[Pretrain 60]\n",
      "\t(train) loss : 152.68644,\tacc : 0.38857\n",
      "\t(eval) loss : 142.79944,\tacc : 0.40861\n",
      "(6.07s)/[Pretrain 61]\n",
      "\t(train) loss : 151.19603,\tacc : 0.40571\n",
      "\t(eval) loss : 142.60113,\tacc : 0.41225\n",
      "(5.98s)/[Pretrain 62]\n",
      "\t(train) loss : 152.01643,\tacc : 0.36571\n",
      "\t(eval) loss : 142.17718,\tacc : 0.40543\n",
      "(6.45s)/[Pretrain 63]\n",
      "\t(train) loss : 149.37002,\tacc : 0.40571\n",
      "\t(eval) loss : 141.81471,\tacc : 0.40736\n",
      "(6.35s)/[Pretrain 64]\n",
      "\t(train) loss : 150.29623,\tacc : 0.40000\n",
      "\t(eval) loss : 141.31364,\tacc : 0.41009\n",
      "(6.17s)/[Pretrain 65]\n",
      "\t(train) loss : 150.23943,\tacc : 0.36571\n",
      "\t(eval) loss : 141.56019,\tacc : 0.40543\n",
      "(6.45s)/[Pretrain 66]\n",
      "\t(train) loss : 149.32150,\tacc : 0.42286\n",
      "\t(eval) loss : 141.28884,\tacc : 0.40452\n",
      "(6.32s)/[Pretrain 67]\n",
      "\t(train) loss : 148.21691,\tacc : 0.40571\n",
      "\t(eval) loss : 141.01845,\tacc : 0.40520\n",
      "(6.58s)/[Pretrain 68]\n",
      "\t(train) loss : 147.96542,\tacc : 0.43429\n",
      "\t(eval) loss : 140.52554,\tacc : 0.40623\n",
      "(6.99s)/[Pretrain 69]\n",
      "\t(train) loss : 147.39279,\tacc : 0.45714\n",
      "\t(eval) loss : 139.78195,\tacc : 0.41122\n",
      "(6.12s)/[Pretrain 70]\n",
      "\t(train) loss : 147.26479,\tacc : 0.38286\n",
      "\t(eval) loss : 139.19287,\tacc : 0.41145\n",
      "(6.00s)/[Pretrain 71]\n",
      "\t(train) loss : 145.85984,\tacc : 0.42286\n",
      "\t(eval) loss : 138.96400,\tacc : 0.40986\n",
      "(6.44s)/[Pretrain 72]\n",
      "\t(train) loss : 146.47789,\tacc : 0.40000\n",
      "\t(eval) loss : 138.57373,\tacc : 0.41304\n",
      "(7.56s)/[Pretrain 73]\n",
      "\t(train) loss : 146.41940,\tacc : 0.38857\n",
      "\t(eval) loss : 137.60452,\tacc : 0.41679\n",
      "(6.48s)/[Pretrain 74]\n",
      "\t(train) loss : 145.26605,\tacc : 0.40571\n",
      "\t(eval) loss : 137.29576,\tacc : 0.41509\n",
      "(6.70s)/[Pretrain 75]\n",
      "\t(train) loss : 144.19104,\tacc : 0.45714\n",
      "\t(eval) loss : 136.68948,\tacc : 0.41463\n",
      "(6.62s)/[Pretrain 76]\n",
      "\t(train) loss : 143.35468,\tacc : 0.45143\n",
      "\t(eval) loss : 136.97051,\tacc : 0.41486\n",
      "(6.25s)/[Pretrain 77]\n",
      "\t(train) loss : 143.57448,\tacc : 0.41714\n",
      "\t(eval) loss : 135.52093,\tacc : 0.41736\n",
      "(6.70s)/[Pretrain 78]\n",
      "\t(train) loss : 141.69014,\tacc : 0.40571\n",
      "\t(eval) loss : 134.59295,\tacc : 0.42838\n",
      "(6.42s)/[Pretrain 79]\n",
      "\t(train) loss : 140.91004,\tacc : 0.42286\n",
      "\t(eval) loss : 135.09781,\tacc : 0.42508\n",
      "(6.36s)/[Pretrain 80]\n",
      "\t(train) loss : 141.34717,\tacc : 0.44571\n",
      "\t(eval) loss : 135.09340,\tacc : 0.41838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6.93s)/[Pretrain 81]\n",
      "\t(train) loss : 138.17201,\tacc : 0.44000\n",
      "\t(eval) loss : 133.55121,\tacc : 0.42542\n",
      "(6.32s)/[Pretrain 82]\n",
      "\t(train) loss : 138.58436,\tacc : 0.42857\n",
      "\t(eval) loss : 132.41676,\tacc : 0.46064\n",
      "(6.47s)/[Pretrain 83]\n",
      "\t(train) loss : 135.79416,\tacc : 0.53143\n",
      "\t(eval) loss : 131.48564,\tacc : 0.48506\n",
      "(6.62s)/[Pretrain 84]\n",
      "\t(train) loss : 138.80917,\tacc : 0.44000\n",
      "\t(eval) loss : 130.16546,\tacc : 0.45394\n",
      "(6.37s)/[Pretrain 85]\n",
      "\t(train) loss : 135.47028,\tacc : 0.45143\n",
      "\t(eval) loss : 129.99463,\tacc : 0.45791\n",
      "(6.31s)/[Pretrain 86]\n",
      "\t(train) loss : 134.63345,\tacc : 0.49143\n",
      "\t(eval) loss : 130.40325,\tacc : 0.48109\n",
      "(6.15s)/[Pretrain 87]\n",
      "\t(train) loss : 133.26303,\tacc : 0.54857\n",
      "\t(eval) loss : 128.52762,\tacc : 0.49665\n",
      "(6.09s)/[Pretrain 88]\n",
      "\t(train) loss : 135.72044,\tacc : 0.47429\n",
      "\t(eval) loss : 127.19855,\tacc : 0.49869\n",
      "(6.01s)/[Pretrain 89]\n",
      "\t(train) loss : 131.04488,\tacc : 0.53714\n",
      "\t(eval) loss : 128.43711,\tacc : 0.52721\n",
      "(6.15s)/[Pretrain 90]\n",
      "\t(train) loss : 134.15120,\tacc : 0.48571\n",
      "\t(eval) loss : 127.27146,\tacc : 0.50199\n",
      "(6.02s)/[Pretrain 91]\n",
      "\t(train) loss : 132.43579,\tacc : 0.49143\n",
      "\t(eval) loss : 126.57168,\tacc : 0.52039\n",
      "(6.08s)/[Pretrain 92]\n",
      "\t(train) loss : 127.63371,\tacc : 0.54286\n",
      "\t(eval) loss : 125.32335,\tacc : 0.53436\n",
      "(6.11s)/[Pretrain 93]\n",
      "\t(train) loss : 129.95627,\tacc : 0.53143\n",
      "\t(eval) loss : 124.52244,\tacc : 0.52914\n",
      "(6.09s)/[Pretrain 94]\n",
      "\t(train) loss : 130.17124,\tacc : 0.53143\n",
      "\t(eval) loss : 126.35089,\tacc : 0.55436\n",
      "(6.06s)/[Pretrain 95]\n",
      "\t(train) loss : 134.16448,\tacc : 0.54286\n",
      "\t(eval) loss : 126.01964,\tacc : 0.47893\n",
      "(6.47s)/[Pretrain 96]\n",
      "\t(train) loss : 126.30805,\tacc : 0.54286\n",
      "\t(eval) loss : 129.03698,\tacc : 0.55481\n",
      "(6.35s)/[Pretrain 97]\n",
      "\t(train) loss : 131.90257,\tacc : 0.49143\n",
      "\t(eval) loss : 125.34481,\tacc : 0.46189\n",
      "(5.96s)/[Pretrain 98]\n",
      "\t(train) loss : 126.07342,\tacc : 0.50857\n",
      "\t(eval) loss : 125.60145,\tacc : 0.54936\n",
      "(6.01s)/[Pretrain 99]\n",
      "\t(train) loss : 133.31421,\tacc : 0.56571\n",
      "\t(eval) loss : 122.87488,\tacc : 0.54175\n",
      "(6.39s)/[Pretrain 100]\n",
      "\t(train) loss : 128.66611,\tacc : 0.54857\n",
      "\t(eval) loss : 124.54412,\tacc : 0.54459\n",
      "(6.08s)/[Pretrain 101]\n",
      "\t(train) loss : 124.81629,\tacc : 0.55429\n",
      "\t(eval) loss : 124.10289,\tacc : 0.52743\n",
      "(6.13s)/[Pretrain 102]\n",
      "\t(train) loss : 126.20262,\tacc : 0.55429\n",
      "\t(eval) loss : 122.39458,\tacc : 0.53777\n",
      "(6.10s)/[Pretrain 103]\n",
      "\t(train) loss : 127.27672,\tacc : 0.52000\n",
      "\t(eval) loss : 122.20139,\tacc : 0.53664\n",
      "(6.32s)/[Pretrain 104]\n",
      "\t(train) loss : 130.03455,\tacc : 0.52000\n",
      "\t(eval) loss : 122.61872,\tacc : 0.50630\n",
      "(7.50s)/[Pretrain 105]\n",
      "\t(train) loss : 124.20188,\tacc : 0.56571\n",
      "\t(eval) loss : 124.82765,\tacc : 0.55197\n",
      "(6.71s)/[Pretrain 106]\n",
      "\t(train) loss : 128.81865,\tacc : 0.50857\n",
      "\t(eval) loss : 121.48750,\tacc : 0.52096\n",
      "(6.18s)/[Pretrain 107]\n",
      "\t(train) loss : 122.29251,\tacc : 0.49714\n",
      "\t(eval) loss : 126.07667,\tacc : 0.56799\n",
      "(6.13s)/[Pretrain 108]\n",
      "\t(train) loss : 133.84831,\tacc : 0.46857\n",
      "\t(eval) loss : 124.24733,\tacc : 0.44621\n",
      "(6.42s)/[Pretrain 109]\n",
      "\t(train) loss : 123.00648,\tacc : 0.52571\n",
      "\t(eval) loss : 126.33944,\tacc : 0.56640\n",
      "(6.13s)/[Pretrain 110]\n",
      "\t(train) loss : 131.35580,\tacc : 0.48571\n",
      "\t(eval) loss : 121.47688,\tacc : 0.50483\n",
      "(6.16s)/[Pretrain 111]\n",
      "\t(train) loss : 125.15129,\tacc : 0.55429\n",
      "\t(eval) loss : 122.43274,\tacc : 0.55208\n",
      "(6.46s)/[Pretrain 112]\n",
      "\t(train) loss : 123.59079,\tacc : 0.56571\n",
      "\t(eval) loss : 121.30787,\tacc : 0.53539\n",
      "(5.98s)/[Pretrain 113]\n",
      "\t(train) loss : 124.90065,\tacc : 0.54857\n",
      "\t(eval) loss : 121.35421,\tacc : 0.53732\n",
      "(6.02s)/[Pretrain 114]\n",
      "\t(train) loss : 124.96437,\tacc : 0.56571\n",
      "\t(eval) loss : 120.96347,\tacc : 0.50608\n",
      "(5.92s)/[Pretrain 115]\n",
      "\t(train) loss : 126.33031,\tacc : 0.49143\n",
      "\t(eval) loss : 123.06004,\tacc : 0.54947\n",
      "(5.95s)/[Pretrain 116]\n",
      "\t(train) loss : 128.62349,\tacc : 0.57714\n",
      "\t(eval) loss : 119.60696,\tacc : 0.52459\n",
      "(6.17s)/[Pretrain 117]\n",
      "\t(train) loss : 123.86970,\tacc : 0.54857\n",
      "\t(eval) loss : 121.76738,\tacc : 0.54675\n",
      "(6.42s)/[Pretrain 118]\n",
      "\t(train) loss : 123.66419,\tacc : 0.51429\n",
      "\t(eval) loss : 119.78777,\tacc : 0.52823\n",
      "(6.15s)/[Pretrain 119]\n",
      "\t(train) loss : 122.29469,\tacc : 0.57714\n",
      "\t(eval) loss : 119.55616,\tacc : 0.53823\n",
      "(5.94s)/[Pretrain 120]\n",
      "\t(train) loss : 120.80194,\tacc : 0.56000\n",
      "\t(eval) loss : 121.05283,\tacc : 0.56095\n",
      "(5.99s)/[Pretrain 121]\n",
      "\t(train) loss : 123.93836,\tacc : 0.54857\n",
      "\t(eval) loss : 118.46010,\tacc : 0.54356\n",
      "(6.98s)/[Pretrain 122]\n",
      "\t(train) loss : 123.18746,\tacc : 0.53143\n",
      "\t(eval) loss : 117.96711,\tacc : 0.52823\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './saved_models/Pretrain_best.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d7169d89f801>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;34m'optimizer'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0moptimizer_pretrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     }\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mpretrain_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s/pretrain_best.pth'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mSAVE_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-1a3565a58277>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_idx, trainY, shuffle, epoch, pretrain)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mbest_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mpatience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%s/%s_best.pth'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSAVE_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mpatience\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mTOL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\jh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\jh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\jh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './saved_models/Pretrain_best.pth'"
     ]
    }
   ],
   "source": [
    "if PRETRAIN:\n",
    "    pretrain_param = {\n",
    "        'model' : model,\n",
    "        'shuffle' : pretrain_shuffle,\n",
    "        'epoch' : PRETRAIN_EPOCH,\n",
    "        'pretrain' : True,\n",
    "        'train_idx' : train_idx[MASK],\n",
    "        'trainY' : trainY[MASK],\n",
    "        'optimizer' : optimizer_pretrain\n",
    "    }\n",
    "    train(**pretrain_param)\n",
    "    model.load_state_dict(torch.load('%s/pretrain_best.pth'%SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_param = {\n",
    "    'model' : model,\n",
    "    'shuffle' : train_shuffle,\n",
    "    'epoch' : EPOCH,\n",
    "    'pretrain' : False,\n",
    "    'train_idx' : train_idx,\n",
    "    'trainY' : trainY,\n",
    "    'optimizer' : optimizer\n",
    "}\n",
    "train(**train_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
